{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram tokenization\n",
    "\n",
    "The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetTokenizerFast(name_or_path='xlnet-base-cased', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>', 'additional_special_tokens': ['<eop>', '<eod>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<eod>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus:\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won’t be able to tokenize every word), but for the bigger substrings we’ll only keep the most common ones, so we sort them by frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': 31,\n",
       " 'T': 3,\n",
       " 'h': 9,\n",
       " 'i': 13,\n",
       " 's': 13,\n",
       " 't': 14,\n",
       " 'e': 21,\n",
       " 'H': 2,\n",
       " 'u': 6,\n",
       " 'g': 5,\n",
       " 'n': 11,\n",
       " 'F': 1,\n",
       " 'a': 12,\n",
       " 'c': 3,\n",
       " 'C': 1,\n",
       " 'o': 13,\n",
       " 'r': 9,\n",
       " '.': 4,\n",
       " 'p': 2,\n",
       " 'b': 3,\n",
       " 'k': 3,\n",
       " 'z': 2,\n",
       " 'w': 3,\n",
       " 'v': 1,\n",
       " 'l': 7,\n",
       " 'm': 1,\n",
       " 'f': 1,\n",
       " 'y': 3,\n",
       " ',': 1,\n",
       " 'd': 4,\n",
       " '▁t': 7,\n",
       " 'is': 5,\n",
       " 'er': 5,\n",
       " '▁a': 5,\n",
       " '▁to': 4,\n",
       " 'to': 4,\n",
       " 'en': 4,\n",
       " '▁T': 3,\n",
       " '▁Th': 3,\n",
       " '▁Thi': 3,\n",
       " '▁This': 3,\n",
       " 'Th': 3,\n",
       " 'Thi': 3,\n",
       " 'This': 3,\n",
       " 'hi': 3,\n",
       " 'his': 3,\n",
       " 'th': 3,\n",
       " 'ou': 3,\n",
       " 'se': 3,\n",
       " '▁tok': 3,\n",
       " '▁toke': 3,\n",
       " '▁token': 3,\n",
       " 'tok': 3,\n",
       " 'toke': 3,\n",
       " 'token': 3,\n",
       " 'ok': 3,\n",
       " 'oke': 3,\n",
       " 'oken': 3,\n",
       " 'ke': 3,\n",
       " 'ken': 3,\n",
       " '▁s': 3,\n",
       " 'ra': 3,\n",
       " 'nd': 3,\n",
       " '▁i': 2,\n",
       " '▁is': 2,\n",
       " '▁th': 2,\n",
       " '▁the': 2,\n",
       " 'the': 2,\n",
       " 'he': 2,\n",
       " '▁H': 2,\n",
       " 'in': 2,\n",
       " 'rs': 2,\n",
       " 'te': 2,\n",
       " '▁ab': 2,\n",
       " 'ab': 2,\n",
       " '▁tokeni': 2,\n",
       " '▁tokeniz': 2,\n",
       " 'tokeni': 2,\n",
       " 'tokeniz': 2,\n",
       " 'okeni': 2,\n",
       " 'okeniz': 2,\n",
       " 'keni': 2,\n",
       " 'keniz': 2,\n",
       " 'eni': 2,\n",
       " 'eniz': 2,\n",
       " 'ni': 2,\n",
       " 'niz': 2,\n",
       " 'iz': 2,\n",
       " 'at': 2,\n",
       " 'ti': 2,\n",
       " 'tio': 2,\n",
       " 'tion': 2,\n",
       " 'io': 2,\n",
       " 'ion': 2,\n",
       " 'on': 2,\n",
       " '▁se': 2,\n",
       " 'ho': 2,\n",
       " 'how': 2,\n",
       " 'ow': 2,\n",
       " 'era': 2,\n",
       " 'al': 2,\n",
       " 's.': 2,\n",
       " 'll': 2,\n",
       " 'an': 2,\n",
       " 'and': 2,\n",
       " 'ne': 2,\n",
       " '▁Hu': 1,\n",
       " '▁Hug': 1,\n",
       " '▁Hugg': 1,\n",
       " '▁Huggi': 1,\n",
       " '▁Huggin': 1,\n",
       " '▁Hugging': 1,\n",
       " 'Hu': 1,\n",
       " 'Hug': 1,\n",
       " 'Hugg': 1,\n",
       " 'Huggi': 1,\n",
       " 'Huggin': 1,\n",
       " 'Hugging': 1,\n",
       " 'ug': 1,\n",
       " 'ugg': 1,\n",
       " 'uggi': 1,\n",
       " 'uggin': 1,\n",
       " 'ugging': 1,\n",
       " 'gg': 1,\n",
       " 'ggi': 1,\n",
       " 'ggin': 1,\n",
       " 'gging': 1,\n",
       " 'gi': 1,\n",
       " 'gin': 1,\n",
       " 'ging': 1,\n",
       " 'ing': 1,\n",
       " 'ng': 1,\n",
       " '▁F': 1,\n",
       " '▁Fa': 1,\n",
       " '▁Fac': 1,\n",
       " '▁Face': 1,\n",
       " 'Fa': 1,\n",
       " 'Fac': 1,\n",
       " 'Face': 1,\n",
       " 'ac': 1,\n",
       " 'ace': 1,\n",
       " 'ce': 1,\n",
       " '▁C': 1,\n",
       " '▁Co': 1,\n",
       " '▁Cou': 1,\n",
       " '▁Cour': 1,\n",
       " '▁Cours': 1,\n",
       " '▁Course': 1,\n",
       " '▁Course.': 1,\n",
       " 'Co': 1,\n",
       " 'Cou': 1,\n",
       " 'Cour': 1,\n",
       " 'Cours': 1,\n",
       " 'Course': 1,\n",
       " 'Course.': 1,\n",
       " 'our': 1,\n",
       " 'ours': 1,\n",
       " 'ourse': 1,\n",
       " 'ourse.': 1,\n",
       " 'ur': 1,\n",
       " 'urs': 1,\n",
       " 'urse': 1,\n",
       " 'urse.': 1,\n",
       " 'rse': 1,\n",
       " 'rse.': 1,\n",
       " 'se.': 1,\n",
       " 'e.': 1,\n",
       " '▁c': 1,\n",
       " '▁ch': 1,\n",
       " '▁cha': 1,\n",
       " '▁chap': 1,\n",
       " '▁chapt': 1,\n",
       " '▁chapte': 1,\n",
       " '▁chapter': 1,\n",
       " 'ch': 1,\n",
       " 'cha': 1,\n",
       " 'chap': 1,\n",
       " 'chapt': 1,\n",
       " 'chapte': 1,\n",
       " 'chapter': 1,\n",
       " 'ha': 1,\n",
       " 'hap': 1,\n",
       " 'hapt': 1,\n",
       " 'hapte': 1,\n",
       " 'hapter': 1,\n",
       " 'ap': 1,\n",
       " 'apt': 1,\n",
       " 'apte': 1,\n",
       " 'apter': 1,\n",
       " 'pt': 1,\n",
       " 'pte': 1,\n",
       " 'pter': 1,\n",
       " 'ter': 1,\n",
       " '▁abo': 1,\n",
       " '▁abou': 1,\n",
       " '▁about': 1,\n",
       " 'abo': 1,\n",
       " 'abou': 1,\n",
       " 'about': 1,\n",
       " 'bo': 1,\n",
       " 'bou': 1,\n",
       " 'bout': 1,\n",
       " 'out': 1,\n",
       " 'ut': 1,\n",
       " '▁tokeniza': 1,\n",
       " '▁tokenizat': 1,\n",
       " '▁tokenizati': 1,\n",
       " '▁tokenizatio': 1,\n",
       " '▁tokenization': 1,\n",
       " '▁tokenization.': 1,\n",
       " 'tokeniza': 1,\n",
       " 'tokenizat': 1,\n",
       " 'tokenizati': 1,\n",
       " 'tokenizatio': 1,\n",
       " 'tokenization': 1,\n",
       " 'tokenization.': 1,\n",
       " 'okeniza': 1,\n",
       " 'okenizat': 1,\n",
       " 'okenizati': 1,\n",
       " 'okenizatio': 1,\n",
       " 'okenization': 1,\n",
       " 'okenization.': 1,\n",
       " 'keniza': 1,\n",
       " 'kenizat': 1,\n",
       " 'kenizati': 1,\n",
       " 'kenizatio': 1,\n",
       " 'kenization': 1,\n",
       " 'kenization.': 1,\n",
       " 'eniza': 1,\n",
       " 'enizat': 1,\n",
       " 'enizati': 1,\n",
       " 'enizatio': 1,\n",
       " 'enization': 1,\n",
       " 'enization.': 1,\n",
       " 'niza': 1,\n",
       " 'nizat': 1,\n",
       " 'nizati': 1,\n",
       " 'nizatio': 1,\n",
       " 'nization': 1,\n",
       " 'nization.': 1,\n",
       " 'iza': 1,\n",
       " 'izat': 1,\n",
       " 'izati': 1,\n",
       " 'izatio': 1,\n",
       " 'ization': 1,\n",
       " 'ization.': 1,\n",
       " 'za': 1,\n",
       " 'zat': 1,\n",
       " 'zati': 1,\n",
       " 'zatio': 1,\n",
       " 'zation': 1,\n",
       " 'zation.': 1,\n",
       " 'ati': 1,\n",
       " 'atio': 1,\n",
       " 'ation': 1,\n",
       " 'ation.': 1,\n",
       " 'tion.': 1,\n",
       " 'ion.': 1,\n",
       " 'on.': 1,\n",
       " 'n.': 1,\n",
       " '▁sec': 1,\n",
       " '▁sect': 1,\n",
       " '▁secti': 1,\n",
       " '▁sectio': 1,\n",
       " '▁section': 1,\n",
       " 'sec': 1,\n",
       " 'sect': 1,\n",
       " 'secti': 1,\n",
       " 'sectio': 1,\n",
       " 'section': 1,\n",
       " 'ec': 1,\n",
       " 'ect': 1,\n",
       " 'ecti': 1,\n",
       " 'ectio': 1,\n",
       " 'ection': 1,\n",
       " 'ct': 1,\n",
       " 'cti': 1,\n",
       " 'ctio': 1,\n",
       " 'ction': 1,\n",
       " '▁sh': 1,\n",
       " '▁sho': 1,\n",
       " '▁show': 1,\n",
       " '▁shows': 1,\n",
       " 'sh': 1,\n",
       " 'sho': 1,\n",
       " 'show': 1,\n",
       " 'shows': 1,\n",
       " 'hows': 1,\n",
       " 'ows': 1,\n",
       " 'ws': 1,\n",
       " '▁sev': 1,\n",
       " '▁seve': 1,\n",
       " '▁sever': 1,\n",
       " '▁severa': 1,\n",
       " '▁several': 1,\n",
       " 'sev': 1,\n",
       " 'seve': 1,\n",
       " 'sever': 1,\n",
       " 'severa': 1,\n",
       " 'several': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We group the characters with the best subwords to arrive at an initial vocabulary of size 300:\n",
    "\n",
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it’s more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "{'▁': 2.952892114877499, 'T': 5.288267030694535, 'h': 4.189654742026425, 'i': 3.821929961901108, 's': 3.821929961901108, 't': 3.7478219897473863, 'e': 3.342356881639222, 'H': 5.6937321388027, 'u': 4.59511985013459, 'g': 4.777441406928545, 'n': 3.9889840465642745, 'F': 6.386879319362645, 'a': 3.9019726695746444, 'c': 5.288267030694535, 'C': 6.386879319362645, 'o': 3.821929961901108, 'r': 4.189654742026425, '.': 5.000584958242754, 'p': 5.6937321388027, 'b': 5.288267030694535, 'k': 5.288267030694535, 'z': 5.6937321388027, 'w': 5.288267030694535, 'v': 6.386879319362645, 'l': 4.440969170307332, 'm': 6.386879319362645, 'f': 6.386879319362645, 'y': 5.288267030694535, ',': 6.386879319362645, 'd': 5.000584958242754, '▁t': 4.440969170307332, 'is': 4.777441406928545, 'er': 4.777441406928545, '▁a': 4.777441406928545, '▁to': 5.000584958242754, 'to': 5.000584958242754, 'en': 5.000584958242754, '▁T': 5.288267030694535, '▁Th': 5.288267030694535, '▁Thi': 5.288267030694535, '▁This': 5.288267030694535, 'Th': 5.288267030694535, 'Thi': 5.288267030694535, 'This': 5.288267030694535, 'hi': 5.288267030694535, 'his': 5.288267030694535, 'th': 5.288267030694535, 'ou': 5.288267030694535, 'se': 5.288267030694535, '▁tok': 5.288267030694535, '▁toke': 5.288267030694535, '▁token': 5.288267030694535, 'tok': 5.288267030694535, 'toke': 5.288267030694535, 'token': 5.288267030694535, 'ok': 5.288267030694535, 'oke': 5.288267030694535, 'oken': 5.288267030694535, 'ke': 5.288267030694535, 'ken': 5.288267030694535, '▁s': 5.288267030694535, 'ra': 5.288267030694535, 'nd': 5.288267030694535, '▁i': 5.6937321388027, '▁is': 5.6937321388027, '▁th': 5.6937321388027, '▁the': 5.6937321388027, 'the': 5.6937321388027, 'he': 5.6937321388027, '▁H': 5.6937321388027, 'in': 5.6937321388027, 'rs': 5.6937321388027, 'te': 5.6937321388027, '▁ab': 5.6937321388027, 'ab': 5.6937321388027, '▁tokeni': 5.6937321388027, '▁tokeniz': 5.6937321388027, 'tokeni': 5.6937321388027, 'tokeniz': 5.6937321388027, 'okeni': 5.6937321388027, 'okeniz': 5.6937321388027, 'keni': 5.6937321388027, 'keniz': 5.6937321388027, 'eni': 5.6937321388027, 'eniz': 5.6937321388027, 'ni': 5.6937321388027, 'niz': 5.6937321388027, 'iz': 5.6937321388027, 'at': 5.6937321388027, 'ti': 5.6937321388027, 'tio': 5.6937321388027, 'tion': 5.6937321388027, 'io': 5.6937321388027, 'ion': 5.6937321388027, 'on': 5.6937321388027, '▁se': 5.6937321388027, 'ho': 5.6937321388027, 'how': 5.6937321388027, 'ow': 5.6937321388027, 'era': 5.6937321388027, 'al': 5.6937321388027, 's.': 5.6937321388027, 'll': 5.6937321388027, 'an': 5.6937321388027, 'and': 5.6937321388027, 'ne': 5.6937321388027, '▁Hu': 6.386879319362645, '▁Hug': 6.386879319362645, '▁Hugg': 6.386879319362645, '▁Huggi': 6.386879319362645, '▁Huggin': 6.386879319362645, '▁Hugging': 6.386879319362645, 'Hu': 6.386879319362645, 'Hug': 6.386879319362645, 'Hugg': 6.386879319362645, 'Huggi': 6.386879319362645, 'Huggin': 6.386879319362645, 'Hugging': 6.386879319362645, 'ug': 6.386879319362645, 'ugg': 6.386879319362645, 'uggi': 6.386879319362645, 'uggin': 6.386879319362645, 'ugging': 6.386879319362645, 'gg': 6.386879319362645, 'ggi': 6.386879319362645, 'ggin': 6.386879319362645, 'gging': 6.386879319362645, 'gi': 6.386879319362645, 'gin': 6.386879319362645, 'ging': 6.386879319362645, 'ing': 6.386879319362645, 'ng': 6.386879319362645, '▁F': 6.386879319362645, '▁Fa': 6.386879319362645, '▁Fac': 6.386879319362645, '▁Face': 6.386879319362645, 'Fa': 6.386879319362645, 'Fac': 6.386879319362645, 'Face': 6.386879319362645, 'ac': 6.386879319362645, 'ace': 6.386879319362645, 'ce': 6.386879319362645, '▁C': 6.386879319362645, '▁Co': 6.386879319362645, '▁Cou': 6.386879319362645, '▁Cour': 6.386879319362645, '▁Cours': 6.386879319362645, '▁Course': 6.386879319362645, '▁Course.': 6.386879319362645, 'Co': 6.386879319362645, 'Cou': 6.386879319362645, 'Cour': 6.386879319362645, 'Cours': 6.386879319362645, 'Course': 6.386879319362645, 'Course.': 6.386879319362645, 'our': 6.386879319362645, 'ours': 6.386879319362645, 'ourse': 6.386879319362645, 'ourse.': 6.386879319362645, 'ur': 6.386879319362645, 'urs': 6.386879319362645, 'urse': 6.386879319362645, 'urse.': 6.386879319362645, 'rse': 6.386879319362645, 'rse.': 6.386879319362645, 'se.': 6.386879319362645, 'e.': 6.386879319362645, '▁c': 6.386879319362645, '▁ch': 6.386879319362645, '▁cha': 6.386879319362645, '▁chap': 6.386879319362645, '▁chapt': 6.386879319362645, '▁chapte': 6.386879319362645, '▁chapter': 6.386879319362645, 'ch': 6.386879319362645, 'cha': 6.386879319362645, 'chap': 6.386879319362645, 'chapt': 6.386879319362645, 'chapte': 6.386879319362645, 'chapter': 6.386879319362645, 'ha': 6.386879319362645, 'hap': 6.386879319362645, 'hapt': 6.386879319362645, 'hapte': 6.386879319362645, 'hapter': 6.386879319362645, 'ap': 6.386879319362645, 'apt': 6.386879319362645, 'apte': 6.386879319362645, 'apter': 6.386879319362645, 'pt': 6.386879319362645, 'pte': 6.386879319362645, 'pter': 6.386879319362645, 'ter': 6.386879319362645, '▁abo': 6.386879319362645, '▁abou': 6.386879319362645, '▁about': 6.386879319362645, 'abo': 6.386879319362645, 'abou': 6.386879319362645, 'about': 6.386879319362645, 'bo': 6.386879319362645, 'bou': 6.386879319362645, 'bout': 6.386879319362645, 'out': 6.386879319362645, 'ut': 6.386879319362645, '▁tokeniza': 6.386879319362645, '▁tokenizat': 6.386879319362645, '▁tokenizati': 6.386879319362645, '▁tokenizatio': 6.386879319362645, '▁tokenization': 6.386879319362645, '▁tokenization.': 6.386879319362645, 'tokeniza': 6.386879319362645, 'tokenizat': 6.386879319362645, 'tokenizati': 6.386879319362645, 'tokenizatio': 6.386879319362645, 'tokenization': 6.386879319362645, 'tokenization.': 6.386879319362645, 'okeniza': 6.386879319362645, 'okenizat': 6.386879319362645, 'okenizati': 6.386879319362645, 'okenizatio': 6.386879319362645, 'okenization': 6.386879319362645, 'okenization.': 6.386879319362645, 'keniza': 6.386879319362645, 'kenizat': 6.386879319362645, 'kenizati': 6.386879319362645, 'kenizatio': 6.386879319362645, 'kenization': 6.386879319362645, 'kenization.': 6.386879319362645, 'eniza': 6.386879319362645, 'enizat': 6.386879319362645, 'enizati': 6.386879319362645, 'enizatio': 6.386879319362645, 'enization': 6.386879319362645, 'enization.': 6.386879319362645, 'niza': 6.386879319362645, 'nizat': 6.386879319362645, 'nizati': 6.386879319362645, 'nizatio': 6.386879319362645, 'nization': 6.386879319362645, 'nization.': 6.386879319362645, 'iza': 6.386879319362645, 'izat': 6.386879319362645, 'izati': 6.386879319362645, 'izatio': 6.386879319362645, 'ization': 6.386879319362645, 'ization.': 6.386879319362645, 'za': 6.386879319362645, 'zat': 6.386879319362645, 'zati': 6.386879319362645, 'zatio': 6.386879319362645, 'zation': 6.386879319362645, 'zation.': 6.386879319362645, 'ati': 6.386879319362645, 'atio': 6.386879319362645, 'ation': 6.386879319362645, 'ation.': 6.386879319362645, 'tion.': 6.386879319362645, 'ion.': 6.386879319362645, 'on.': 6.386879319362645, 'n.': 6.386879319362645, '▁sec': 6.386879319362645, '▁sect': 6.386879319362645, '▁secti': 6.386879319362645, '▁sectio': 6.386879319362645, '▁section': 6.386879319362645, 'sec': 6.386879319362645, 'sect': 6.386879319362645, 'secti': 6.386879319362645, 'sectio': 6.386879319362645, 'section': 6.386879319362645, 'ec': 6.386879319362645, 'ect': 6.386879319362645, 'ecti': 6.386879319362645, 'ectio': 6.386879319362645, 'ection': 6.386879319362645, 'ct': 6.386879319362645, 'cti': 6.386879319362645, 'ctio': 6.386879319362645, 'ction': 6.386879319362645, '▁sh': 6.386879319362645, '▁sho': 6.386879319362645, '▁show': 6.386879319362645, '▁shows': 6.386879319362645, 'sh': 6.386879319362645, 'sho': 6.386879319362645, 'show': 6.386879319362645, 'shows': 6.386879319362645, 'hows': 6.386879319362645, 'ows': 6.386879319362645, 'ws': 6.386879319362645, '▁sev': 6.386879319362645, '▁seve': 6.386879319362645, '▁sever': 6.386879319362645, '▁severa': 6.386879319362645, '▁several': 6.386879319362645, 'sev': 6.386879319362645, 'seve': 6.386879319362645, 'sever': 6.386879319362645, 'severa': 6.386879319362645, 'several': 6.386879319362645}\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = 0\n",
    "for token, freq in token_freqs.items():\n",
    "    total_sum += freq\n",
    "\n",
    "print(total_sum)\n",
    "\n",
    "model = {}\n",
    "for token, freq in token_freqs.items():\n",
    "    probability = freq / total_sum\n",
    "    negative_log_probability = -log(probability)\n",
    "    model[token] = negative_log_probability\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named best_segmentations. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.\n",
    "\n",
    "Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in best_segmentations.\n",
    "\n",
    "Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "# We can already try our initial model on some words:\n",
    "\n",
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it’s easy to compute the loss of the model on the corpus!\n",
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "# We can check it works on the model we have:\n",
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing the scores for each token is not very hard either; \n",
    "# we just have to compute the loss for the models obtained by deleting each token:\n",
    "\n",
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "# We can try it on a given token:\n",
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \"ll\" is used in the tokenization of \"Hopefully\", and removing it will probably make us use the token \"l\" twice instead, we expect it will have a positive loss. \"his\" is only used inside the word \"This\", which is tokenized as itself, so we expect it to have a zero loss. Here are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to tokenize some text, we just need to apply the pre-tokenization and then use our encode_word() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
