{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@hammadhaideerr/how-to-quantize-a-language-model-to-int8-using-optimum-2fa306767338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/My Drive/Hassan USB Data/Bytewise NLP/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/owner/My Drive/Hassan USB Data/Bytewise NLP/env/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/owner/My Drive/Hassan USB Data/Bytewise NLP/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertModel.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Export the model to ONNX with opset 14\n",
    "onnx_model_path = \"Dynamic Quantization/distilbert_base_uncased.onnx\"\n",
    "dummy_input = torch.randint(0, 100, (1, 10))  # Dummy input for ONNX export\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, opset_version=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Perform dynamic quantization on the exported ONNX model\n",
    "quantized_model_path = \"Dynamic Quantization/quantized_distilbert_base_uncased.onnx\"\n",
    "quantize_dynamic(onnx_model_path, quantized_model_path, weight_type=QuantType.QInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized DistilBERT model saved to Dynamic Quantization/quantized_distilbert_base_uncased.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the quantized model to verify it was saved correctly\n",
    "quantized_model = onnx.load(quantized_model_path)\n",
    "onnx.checker.check_model(quantized_model)\n",
    "\n",
    "print(f\"Quantized DistilBERT model saved to {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@shehrozaslam1317/converting-a-language-model-to-int8-using-optimum-56e318535424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/My Drive/Hassan USB Data/Bytewise NLP/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.33.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/My Drive/Hassan USB Data/Bytewise NLP/env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "onnx_model_path = \"Dynamic Quantization 2/model.onnx\"\n",
    "dummy_input = (\n",
    "    torch.randint(0, 100, (1, 10)),  # input_ids\n",
    "    torch.randint(0, 2, (1, 10))  # attention_mask\n",
    ")\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    opset_version=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantized_model_path = \"Dynamic Quantization 2/Quantized_model.onnx\"\n",
    "# Perform dynamic quantization\n",
    "quantize_dynamic(\n",
    "    onnx_model_path,\n",
    "    quantized_model_path,\n",
    "    weight_type=QuantType.QInt8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.2679795 , -0.11143361, -0.01467639, ..., -0.14686449,\n",
      "          0.15257041,  0.4456752 ],\n",
      "        [-0.61722964, -0.4335706 , -0.03758816, ..., -0.23236962,\n",
      "          0.46683633,  0.3649221 ],\n",
      "        [-0.41510642, -0.29438594,  0.10258104, ..., -0.05495847,\n",
      "          0.07371355,  0.6923371 ],\n",
      "        ...,\n",
      "        [-0.22827029, -0.12904058,  0.08350199, ...,  0.07397674,\n",
      "          0.06351829,  0.31308138],\n",
      "        [-0.14275408, -0.13677111,  0.22930504, ...,  0.02308208,\n",
      "          0.11602733,  0.22510473],\n",
      "        [-0.17076543, -0.17607294,  0.10926739, ...,  0.15501772,\n",
      "         -0.01684695,  0.4110286 ]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the quantized ONNX model\n",
    "session = InferenceSession(quantized_model_path, providers=['AzureExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input data\n",
    "inputs = tokenizer(\"This is a test sentence\", return_tensors=\"pt\", max_length=10, padding=\"max_length\", truncation=True)\n",
    "input_ids_np = inputs[\"input_ids\"].numpy()\n",
    "attention_mask_np = inputs[\"attention_mask\"].numpy()\n",
    "\n",
    "# Run Inference\n",
    "outputs = session.run(None, {\"input_ids\": input_ids_np, \"attention_mask\": attention_mask_np})\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
